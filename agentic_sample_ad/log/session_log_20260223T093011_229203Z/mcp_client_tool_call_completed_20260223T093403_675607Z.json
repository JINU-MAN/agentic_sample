{
  "ts": "2026-02-23T09:34:03.675607+00:00",
  "level": "INFO",
  "component": "mcp_client",
  "action": "tool_call_completed",
  "direction": "inbound",
  "details": {
    "server_script_path": "C:\\agentic_sample_api\\agentic_sample_ad\\mcp_local\\web_search_server.py",
    "tool_name": "fetch_page",
    "result": {
      "content": [
        {
          "type": "text",
          "text": "{\n  \"ok\": true,\n  \"url\": \"https://www.geeksforgeeks.org/nlp/explanation-of-bert-model-nlp/\",\n  \"final_url\": \"https://www.geeksforgeeks.org/nlp/explanation-of-bert-model-nlp/\",\n  \"status_code\": 200,\n  \"title\": \"BERT Model - NLP - GeeksforGeeks\",\n  \"content\": \"BERT Model - NLP - GeeksforGeeks Courses Tutorials Interview Prep NLP Tutorial Libraries Phases Text Preprosessing Tokenization Lemmatization Word Embeddings Projects Ideas Interview Question NLP Quiz NLP Pipeline DL for NLP BERT Model - NLP Last Updated : 11 Sep, 2025 BERT (Bidirectional Encoder Representations from Transformers) stands as an open-source machine learning framework designed for the natural language processing (NLP). The article aims to explore the architecture, working and applications of BERT. Illustration of BERT Model Use Case What is BERT? BERT (Bidirectional Encoder Representations from Transformers) leverages a transformer-based neural network to understand and generate human-like language. BERT employs an encoder-only architecture. In the original Transformer architecture , there are both encoder and decoder modules. The decision to use an encoder-only architecture in BERT suggests a primary emphasis on understanding input sequences rather than generating output sequences. Traditional language models process text sequentially, either from left to right or right to left. This method limits the model's awareness to the immediate context preceding the target word. BERT uses a bi-directional approach considering both the left and right context of words in a sentence, instead of analyzing the text sequentially, BERT looks at all the words in a sentence simultaneously. Pre-training BERT Model The BERT model undergoes Pre-training on Large amounts of unlabeled text to learn contextual embeddings. BERT is pre-trained on large amount of unlabeled text data. The model learns contextual embeddings, which are the representations of words that take into account their surrounding context in a sentence. BERT engages in various unsupervised pre-training tasks. For instance, it might learn to predict missing words in a sentence (Masked Language Model or MLM task), understand the relationship between two sentences, or predict the next sentence in a pair. Workflow of BERT BERT is designed to generate a language model so, only the encoder mechanism is used. Sequence of tokens are fed to the Transformer encoder. These tokens are first embedded into vectors and then processed in the neural network. The output is a sequence of vectors, each corresponding to an input token, providing contextualized representations. When training language models, defining a prediction goal is a challenge. Many models predict the next word in a sequence, which is a directional approach and may limit context learning. BERT Model Working BERT addresses this challenge with two innovative training strategies: Masked Language Model (MLM) Next Sentence Prediction (NSP) 1. Masked Language Model (MLM) In BERT's pre-training process, a portion of words in each input sequence is masked and the model is trained to predict the original values of these masked words based on the context provided by the surrounding words. BERT adds a classification layer on top of the output from the encoder. This layer is important for predicting the masked words. The output vectors from the classification layer are multiplied by the embedding matrix, transforming them into the vocabulary dimension. This step helps align the predicted representations with the vocabulary space. The probability of each word in the vocabulary is calculated using the SoftMax activation function . This step generates a probability distribution over the entire vocabulary for each masked position. The loss function used during training considers only the prediction of the masked values. The model is penalized for the deviation between its predictions and the actual values of the masked words. The model converges slower than directional models because during training, BERT is only concerned with predicting the masked values, ignoring the prediction of the non-masked words. The increased context awareness achieved through this strategy compensates for the slower convergence. 2. Next Sentence Prediction (NSP) BERT predicts if the second sentence is connected to the first. This is done by transforming the output of the [CLS] token into a 2×1 shaped vector using a classification layer, and then calculating the probability of whether the second sentence follows the first using SoftMax. In the training process, BERT learns to understand the relationship between pairs of sentences, predicting if the second sentence follows the first in the original document. 50% of the input pairs have the second sentence as the subsequent sentence in the original document, and the other 50% have a randomly chosen sentence. To help the model distinguish between connected and disconnected sentence pairs. The input is processed before entering the model. BERT predicts if the second sentence is connected to the first. This is done by transforming the output of the [CLS] token into a 2×1 shaped vector using a classification layer, and then calculating the probability of whether the second sentence follows the first using SoftMax. During the training of BERT model, the Masked LM and Next Sentence Prediction are trained together. The model aims to minimize the combined loss function of the Masked LM and Next Sentence Prediction, leading to a robust language model with enhanced capabilities in understanding context within sentences and relationships between sentences. Why to train Masked LM and Next Sentence Prediction together? Masked LM helps BERT to understand the context within a sentence and Next Sentence Prediction helps BERT grasp the connection or relationship between pairs of sentences. Hence, training both the strategies together ensures that BERT learns a broad and comprehensive understanding of language, capturing both details within sentences and the flow between sentences. Fine-Tuning on Labeled Data We perform Fine-tuning on labeled data for specific NLP tasks. After the pre-training phase, the BERT model, armed with its contextual embeddings, is fine-tuned for specific natura\",\n  \"truncated\": true,\n  \"content_length\": 17939\n}"
        }
      ],
      "structuredContent": {
        "result": {
          "ok": true,
          "url": "https://www.geeksforgeeks.org/nlp/explanation-of-bert-model-nlp/",
          "final_url": "https://www.geeksforgeeks.org/nlp/explanation-of-bert-model-nlp/",
          "status_code": 200,
          "title": "BERT Model - NLP - GeeksforGeeks",
          "content": "BERT Model - NLP - GeeksforGeeks Courses Tutorials Interview Prep NLP Tutorial Libraries Phases Text Preprosessing Tokenization Lemmatization Word Embeddings Projects Ideas Interview Question NLP Quiz NLP Pipeline DL for NLP BERT Model - NLP Last Updated : 11 Sep, 2025 BERT (Bidirectional Encoder Representations from Transformers) stands as an open-source machine learning framework designed for the natural language processing (NLP). The article aims to explore the architecture, working and applications of BERT. Illustration of BERT Model Use Case What is BERT? BERT (Bidirectional Encoder Representations from Transformers) leverages a transformer-based neural network to understand and generate human-like language. BERT employs an encoder-only architecture. In the original Transformer architecture , there are both encoder and decoder modules. The decision to use an encoder-only architecture in BERT suggests a primary emphasis on understanding input sequences rather than generating output sequences. Traditional language models process text sequentially, either from left to right or right to left. This method limits the model's awareness to the immediate context preceding the target word. BERT uses a bi-directional approach considering both the left and right context of words in a sentence, instead of analyzing the text sequentially, BERT looks at all the words in a sentence simultaneously. Pre-training BERT Model The BERT model undergoes Pre-training on Large amounts of unlabeled text to learn contextual embeddings. BERT is pre-trained on large amount of unlabeled text data. The model learns contextual embeddings, which are the representations of words that take into account their surrounding context in a sentence. BERT engages in various unsupervised pre-training tasks. For instance, it might learn to predict missing words in a sentence (Masked Language Model or MLM task), understand the relationship between two sentences, or predict the next sentence in a pair. Workflow of BERT BERT is designed to generate a language model so, only the encoder mechanism is used. Sequence of tokens are fed to the Transformer encoder. These tokens are first embedded into vectors and then processed in the neural network. The output is a sequence of vectors, each corresponding to an input token, providing contextualized representations. When training language models, defining a prediction goal is a challenge. Many models predict the next word in a sequence, which is a directional approach and may limit context learning. BERT Model Working BERT addresses this challenge with two innovative training strategies: Masked Language Model (MLM) Next Sentence Prediction (NSP) 1. Masked Language Model (MLM) In BERT's pre-training process, a portion of words in each input sequence is masked and the model is trained to predict the original values of these masked words based on the context provided by the surrounding words. BERT adds a classification layer on top of the output from the encoder. This layer is important for predicting the masked words. The output vectors from the classification layer are multiplied by the embedding matrix, transforming them into the vocabulary dimension. This step helps align the predicted representations with the vocabulary space. The probability of each word in the vocabulary is calculated using the SoftMax activation function . This step generates a probability distribution over the entire vocabulary for each masked position. The loss function used during training considers only the prediction of the masked values. The model is penalized for the deviation between its predictions and the actual values of the masked words. The model converges slower than directional models because during training, BERT is only concerned with predicting the masked values, ignoring the prediction of the non-masked words. The increased context awareness achieved through this strategy compensates for the slower convergence. 2. Next Sentence Prediction (NSP) BERT predicts if the second sentence is connected to the first. This is done by transforming the output of the [CLS] token into a 2×1 shaped vector using a classification layer, and then calculating the probability of whether the second sentence follows the first using SoftMax. In the training process, BERT learns to understand the relationship between pairs of sentences, predicting if the second sentence follows the first in the original document. 50% of the input pairs have the second sentence as the subsequent sentence in the original document, and the other 50% have a randomly chosen sentence. To help the model distinguish between connected and disconnected sentence pairs. The input is processed before entering the model. BERT predicts if the second sentence is connected to the first. This is done by transforming the output of the [CLS] token into a 2×1 shaped vector using a classification layer, and then calculating the probability of whether the second sentence follows the first using SoftMax. During the training of BERT model, the Masked LM and Next Sentence Prediction are trained together. The model aims to minimize the combined loss function of the Masked LM and Next Sentence Prediction, leading to a robust language model with enhanced capabilities in understanding context within sentences and relationships between sentences. Why to train Masked LM and Next Sentence Prediction together? Masked LM helps BERT to understand the context within a sentence and Next Sentence Prediction helps BERT grasp the connection or relationship between pairs of sentences. Hence, training both the strategies together ensures that BERT learns a broad and comprehensive understanding of language, capturing both details within sentences and the flow between sentences. Fine-Tuning on Labeled Data We perform Fine-tuning on labeled data for specific NLP tasks. After the pre-training phase, the BERT model, armed with its contextual embeddings, is fine-tuned for specific natura",
          "truncated": true,
          "content_length": 17939
        }
      },
      "isError": false
    }
  }
}
