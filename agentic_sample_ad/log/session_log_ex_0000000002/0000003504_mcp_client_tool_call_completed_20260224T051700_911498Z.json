{
  "ts": "2026-02-24T05:17:00.911498+00:00",
  "level": "INFO",
  "component": "mcp_client",
  "action": "tool_call_completed",
  "direction": "inbound",
  "details": {
    "server_script_path": "C:\\agentic_sample_api\\agentic_sample_ad\\mcp_local\\paper_server.py",
    "tool_name": "get_paper_head",
    "result": {
      "content": [
        {
          "type": "text",
          "text": "{\n  \"ok\": true,\n  \"path\": \"C:\\\\agentic_sample_api\\\\agentic_sample_ad\\\\db\\\\paper\\\\AE_ An Asymmetric Extremum Content Defined Chunking Algorithm for Fast and Bandwidth-Efficient Data Deduplication.pdf\",\n  \"filename\": \"AE_ An Asymmetric Extremum Content Defined Chunking Algorithm for Fast and Bandwidth-Efficient Data Deduplication.pdf\",\n  \"char_count\": 16000,\n  \"head_full_char_count\": 24106,\n  \"head_max_pages\": 4,\n  \"truncated\": true,\n  \"content\": \"AE: An Asymmetric Extremum Content Deﬁned\\nChunking Algorithm for Fast and\\nBandwidth-Efﬁcient Data Deduplication\\nY ucheng Zhangy, Hong Jiang z, Dan Feng y*, Wen Xia y, Min Fu y, Fangting Huang y, Y ukun Zhouy\\nyWuhan National Laboratory for Optoelectronics\\nSchool of Computer, Huazhong University of Science and Technology, Wuhan, China\\nzUniversity of Nebraska-Lincoln, Lincoln, NE, USA\\n*Corresponding author: dfeng@hust.edu.cn\\nAbstract—Data deduplication, a space-efﬁcient and\\nbandwidth-saving technology, plays an important role in\\nbandwidth-efﬁcient data transmission in various data-intensive\\nnetwork and cloud applications. Rabin-based and MAXP-based\\nContent-Deﬁned Chunking (CDC) algorithms, while robust\\nin ﬁnding suitable cut-points for chunk-level redundancy\\nelimination, face the key challenges of (1) low chunking\\nthroughput that renders the chunking stage the deduplication\\nperformance bottleneck and (2) large chunk-size variance that\\ndecreases deduplication efﬁciency. To address these challenges,\\nthis paper proposes a new CDC algorithm called the Asymmetric\\nExtremum (AE) algorithm. The main idea behind AE is based\\non the observation that the extreme value in an asymmetric local\\nrange is not likely to be replaced by a new extreme value in\\ndealing with the boundaries-shift problem, which motivates AE’s\\nuse of asymmetric (rather than symmetric as in MAXP) local\\nrange to identify cut-points and simultaneously achieve high\\nchunking throughput and low chunk-size variance. As a result,\\nAE simultaneously addresses the problems of low chunking\\nthroughput in MAXP and Rabin and high chunk-size variance\\nin Rabin. The experimental results based on four real-world\\ndatasets show that AE improves the throughput performance\\nof the state-of-the-art CDC algorithms by 3x while attaining\\ncomparable or higher deduplication efﬁciency.\\nI. I NTRODUCTION\\nAccording to a study of International Data Corporation (ID-\\nC), the amount of digital information generated in the whole\\nworld is about 1.8ZB in 2012, and that amount will reach 40ZB\\nby 2020 [1]. How to efﬁciently store and transfer such large\\nvolumes of digital data is a challenging problem. Moreover,\\nIDC also shows that about three quarters of digital information\\nis duplicated. As a result, data deduplication, a space and\\nbandwidth efﬁcient technology that prevents redundant data\\nfrom being stored in storage devices and transmitted over the\\nnetworks, is one of the most important methods to tackle\\nthis challenge. Due to its signiﬁcant data reduction efﬁciency,\\nchunk-level deduplication is used in various ﬁelds, such as\\nstorage systems [2], [3], Redundancy Elimination (RE) in\\nnetworks [4], [5], ﬁle-transfer systems (rsync [6]) and remote-\\nﬁle systems (LBFS [7]).\\nChunk-level deduplication schemes divide the input data\\nstream into chunks and then hash each chunk to generate its\\nﬁngerprint that uniquely identiﬁes the chunk. Duplicate chunks\\ncan be removed if their ﬁngerprints are matched with those of\\npreviously stored or transmitted chunks. As the ﬁrst and key\\nstage in the chunk-level deduplication workﬂow, the chunking\\nalgorithm is responsible for dividing the input data stream into\\nchunks of either ﬁxed size or variable size for redundancy de-\\ntection. Fix-Sized Chunking (FSC) [8] marks chunks’ bound-\\naries by their positions and thus is simple and extremely fast.\\nThe main drawback of FSC is its low deduplication efﬁciency\\nthat stems from the boundaries-shift problem. For example, if\\none byte is inserted at the beginning of an input data stream, all\\ncurrent chunk boundaries declared by FSC will be shifted and\\nno duplicate chunks will be identiﬁed and eliminated. Content-\\nDeﬁned Chunking (CDC) divides the input data stream into\\nvariable-sized chunks. It solves the boundaries-shift problem\\nby declaring chunk boundaries depending on local content. As\\na result, the CDC algorithm outperforms the FSC algorithm\\nin terms of deduplication efﬁciency and has been widely used\\nin bandwidth- and storage-efﬁcient applications [9], [10]. To\\nprovide the necessary basis to facilitate the discussion of and\\ncomparison among different CDC algorithms, we list below\\nsome key properties that a desirable CDC algorithm should\\nhave.\\n1) Content deﬁned. To avoid the boundaries-shift problem,\\nthe algorithm should declare the chunk boundaries based\\non local content, i.e., the cut-points for chunking must be\\ncontent deﬁned.\\n2) Low computational overhead. CDC algorithms need to\\ncheck almost every byte in an input data stream to\\nﬁnd the chunk boundaries. This means that the algorith-\\nm execution time is approximately proportional to the\\nnumber of bytes of the input data stream, which can\\ntake up signiﬁcant CPU resources. Hence, in order to\\nachieve higher deduplication throughput, the chunking\\nalgorithm should be simple and devoid of time-consuming\\noperations.\\n3) Small chunk size variability. The variance of chunk size\\nhas a signiﬁcant impact on the deduplication efﬁciency.\\nThe smaller the variance of the chunk size is, the higher\\nthe deduplication efﬁciency will be achieved [11].\\n4) Ability to identify and eliminate low-entropy strings.\\nThe content of real data may sometimes include low-\\nentropy strings [12]. These strings include very few\\ndistinct characters but a large amount of repetitive bytes.\\nIn order to achieve higher deduplication efﬁciency, it is\\ndesirable for the algorithm to be capable of detecting and\\neliminating these duplicate strings.\\n5) Less limitations on chunk size. Minimum and maximum\\n2015 IEEE Conference on Computer Communications (INFOCOM)\\n978-1-4799-8381-0/15/$31.00 ©2015 IEEE 1337 thresholds are often imposed on chunk size to avoid\\nchunks being too short or too long. These measures\\nreduce chunk size variance, but also make the chunk\\nboundaries position-dependent and thus not truly content-\\ndeﬁned, which also reduces the deduplication efﬁciency\\n[13].\\nThe Rabin ﬁngerprint [14] based CDC algorithm (Rabin)\\nis widely employed for redundancy elimination in both storage\\nsystems [15], [2], [16] and networks [5], [17]. The main prob-\\nlems of the Rabin algorithm are its low chunking throughput,\\nwhich renders the chunking process the performance bottle-\\nneck of the deduplication workﬂow [18], [19], and large chunk\\nsize variance that lowers the deduplication efﬁciency [11].\\nMAXP [20] is a CDC approach that addresses the chunk-size\\nvariance problem of Rabin by treating the local extreme values\\nas cut-points. Owing to its smaller chunk size variance and\\nlower memory overhead than Rabin, MAXP was recommended\\nto be used in redundancy elimination in networks [10], [21].\\nMAXP slides a ﬁx-sized symmetric window over the byte\\nstream on a byte-by-byte basis, and checks whether the value\\nof the byte at the center of the current window is the strictly\\nextreme value in the window. The byte found to be the extreme\\nvalue is declared a cut-point (chunk boundary). This strategy\\nof ﬁnding the local maximum values dictates that the MAXP\\nalgorithm recheck some previously compared bytes in the\\nreverse direction of the stream and thus requires more than one\\ncomparison and more than ﬁve conditional branch operations\\nper byte scanned [22], which signiﬁcantly lowers its chunking\\nthroughput.\\nIn other words, while the MAXP algorithm improves\\nthe Rabin algorithm by reducing the chunk-size variance,\\nthe problem of low chunking throughput remains in both\\nalgorithms. To this end, we propose the Asymmetric Ex...(truncated)"
        }
      ],
      "structuredContent": {
        "result": {
          "ok": true,
          "path": "C:\\agentic_sample_api\\agentic_sample_ad\\db\\paper\\AE_ An Asymmetric Extremum Content Defined Chunking Algorithm for Fast and Bandwidth-Efficient Data Deduplication.pdf",
          "filename": "AE_ An Asymmetric Extremum Content Defined Chunking Algorithm for Fast and Bandwidth-Efficient Data Deduplication.pdf",
          "char_count": 16000,
          "head_full_char_count": 24106,
          "head_max_pages": 4,
          "truncated": true,
          "content": "AE: An Asymmetric Extremum Content Deﬁned\nChunking Algorithm for Fast and\nBandwidth-Efﬁcient Data Deduplication\nY ucheng Zhangy, Hong Jiang z, Dan Feng y*, Wen Xia y, Min Fu y, Fangting Huang y, Y ukun Zhouy\nyWuhan National Laboratory for Optoelectronics\nSchool of Computer, Huazhong University of Science and Technology, Wuhan, China\nzUniversity of Nebraska-Lincoln, Lincoln, NE, USA\n*Corresponding author: dfeng@hust.edu.cn\nAbstract—Data deduplication, a space-efﬁcient and\nbandwidth-saving technology, plays an important role in\nbandwidth-efﬁcient data transmission in various data-intensive\nnetwork and cloud applications. Rabin-based and MAXP-based\nContent-Deﬁned Chunking (CDC) algorithms, while robust\nin ﬁnding suitable cut-points for chunk-level redundancy\nelimination, face the key challenges of (1) low chunking\nthroughput that renders the chunking stage the deduplication\nperformance bottleneck and (2) large chunk-size variance that\ndecreases deduplication efﬁciency. To address these challenges,\nthis paper proposes a new CDC algorithm called the Asymmetric\nExtremum (AE) algorithm. The main idea behind AE is based\non the observation that the extreme value in an asymmetric local\nrange is not likely to be replaced by a new extreme value in\ndealing with the boundaries-shift problem, which motivates AE’s\nuse of asymmetric (rather than symmetric as in MAXP) local\nrange to identify cut-points and simultaneously achieve high\nchunking throughput and low chunk-size variance. As a result,\nAE simultaneously addresses the problems of low chunking\nthroughput in MAXP and Rabin and high chunk-size variance\nin Rabin. The experimental results based on four real-world\ndatasets show that AE improves the throughput performance\nof the state-of-the-art CDC algorithms by 3x while attaining\ncomparable or higher deduplication efﬁciency.\nI. I NTRODUCTION\nAccording to a study of International Data Corporation (ID-\nC), the amount of digital information generated in the whole\nworld is about 1.8ZB in 2012, and that amount will reach 40ZB\nby 2020 [1]. How to efﬁciently store and transfer such large\nvolumes of digital data is a challenging problem. Moreover,\nIDC also shows that about three quarters of digital information\nis duplicated. As a result, data deduplication, a space and\nbandwidth efﬁcient technology that prevents redundant data\nfrom being stored in storage devices and transmitted over the\nnetworks, is one of the most important methods to tackle\nthis challenge. Due to its signiﬁcant data reduction efﬁciency,\nchunk-level deduplication is used in various ﬁelds, such as\nstorage systems [2], [3], Redundancy Elimination (RE) in\nnetworks [4], [5], ﬁle-transfer systems (rsync [6]) and remote-\nﬁle systems (LBFS [7]).\nChunk-level deduplication schemes divide the input data\nstream into chunks and then hash each chunk to generate its\nﬁngerprint that uniquely identiﬁes the chunk. Duplicate chunks\ncan be removed if their ﬁngerprints are matched with those of\npreviously stored or transmitted chunks. As the ﬁrst and key\nstage in the chunk-level deduplication workﬂow, the chunking\nalgorithm is responsible for dividing the input data stream into\nchunks of either ﬁxed size or variable size for redundancy de-\ntection. Fix-Sized Chunking (FSC) [8] marks chunks’ bound-\naries by their positions and thus is simple and extremely fast.\nThe main drawback of FSC is its low deduplication efﬁciency\nthat stems from the boundaries-shift problem. For example, if\none byte is inserted at the beginning of an input data stream, all\ncurrent chunk boundaries declared by FSC will be shifted and\nno duplicate chunks will be identiﬁed and eliminated. Content-\nDeﬁned Chunking (CDC) divides the input data stream into\nvariable-sized chunks. It solves the boundaries-shift problem\nby declaring chunk boundaries depending on local content. As\na result, the CDC algorithm outperforms the FSC algorithm\nin terms of deduplication efﬁciency and has been widely used\nin bandwidth- and storage-efﬁcient applications [9], [10]. To\nprovide the necessary basis to facilitate the discussion of and\ncomparison among different CDC algorithms, we list below\nsome key properties that a desirable CDC algorithm should\nhave.\n1) Content deﬁned. To avoid the boundaries-shift problem,\nthe algorithm should declare the chunk boundaries based\non local content, i.e., the cut-points for chunking must be\ncontent deﬁned.\n2) Low computational overhead. CDC algorithms need to\ncheck almost every byte in an input data stream to\nﬁnd the chunk boundaries. This means that the algorith-\nm execution time is approximately proportional to the\nnumber of bytes of the input data stream, which can\ntake up signiﬁcant CPU resources. Hence, in order to\nachieve higher deduplication throughput, the chunking\nalgorithm should be simple and devoid of time-consuming\noperations.\n3) Small chunk size variability. The variance of chunk size\nhas a signiﬁcant impact on the deduplication efﬁciency.\nThe smaller the variance of the chunk size is, the higher\nthe deduplication efﬁciency will be achieved [11].\n4) Ability to identify and eliminate low-entropy strings.\nThe content of real data may sometimes include low-\nentropy strings [12]. These strings include very few\ndistinct characters but a large amount of repetitive bytes.\nIn order to achieve higher deduplication efﬁciency, it is\ndesirable for the algorithm to be capable of detecting and\neliminating these duplicate strings.\n5) Less limitations on chunk size. Minimum and maximum\n2015 IEEE Conference on Computer Communications (INFOCOM)\n978-1-4799-8381-0/15/$31.00 ©2015 IEEE 1337 thresholds are often imposed on chunk size to avoid\nchunks being too short or too long. These measures\nreduce chunk size variance, but also make the chunk\nboundaries position-dependent and thus not truly content-\ndeﬁned, which also reduces the deduplication efﬁciency\n[13].\nThe Rabin ﬁngerprint [14] based CDC algorithm (Rabin)\nis widely employed for redundancy elimination in both storage\nsystems [15], [2], [16] and networks [5], [17]. The main prob-\nlems of the Rabin algorithm are its low chunking throughput,\nwhich renders the chunking process the performance bottle-\nneck of the deduplication workﬂow [18], [19], and large chunk\nsize variance that lowers the deduplication efﬁciency [11].\nMAXP [20] is a CDC approach that addresses the chunk-size\nvariance problem of Rabin by treating the local extreme values\nas cut-points. Owing to its smaller chunk size variance and\nlower memory overhead than Rabin, MAXP was recommended\nto be used in redundancy elimination in networks [10], [21].\nMAXP slides a ﬁx-sized symmetric window over the byte\nstream on a byte-by-byte basis, and checks whether the value\nof the byte at the center of the current window is the strictly\nextreme value in the window. The byte found to be the extreme\nvalue is declared a cut-point (chunk boundary). This strategy\nof ﬁnding the local maximum values dictates that the MAXP\nalgorithm recheck some previously compared bytes in the\nreverse direction of the stream and thus requires more than one\ncomparison and more than ﬁve conditional branch operations\nper byte scanned [22], which signiﬁcantly lowers its chunking\nthroughput.\nIn other words, while the MAXP algorithm improves\nthe Rabin algorithm by reducing the chunk-size variance,\nthe problem of low chunking throughput remains in both\nalgorithms. To this end, we propose the Asymmetric Extremum\nchunking algorithm (AE), a new CDC algorithm that signiﬁ-\ncantly improves the chunking throughput of the above existing\nalgorithms while providing comparable or better deduplication\nefﬁciency by using the local extreme value in a variable-\nsized asymmetric window to overcome the aforementioned\nboundaries-shift problem. With a variable-sized asymmetric\nwindow, instead of a ﬁx-sized symmetric window as in MAXP ,\nthe AE algorithm ﬁnds the extreme value in the window\nwithout having to backtrack and thus requiring only one\ncomparison and two conditional branch operations p...(truncated)"
        }
      },
      "isError": false
    }
  },
  "session_seq": 3504
}
